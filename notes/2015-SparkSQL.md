## 为什么做Spark SQL
扩展关系处理覆盖Spark中原生RDD以及更广泛的数据源
### Goal
1. 支持在Spark内部程序和外部数据源上执行关系处理
2. 使用已有的数据库技术提供高性能
3. 支持新的数据源，比如半结构化数据和外部数据库的联合查询？
4. 能够支持高级分析算法的扩展，比如图处理和机器学习

### Problem
### Solution
- DataFrame API
- Catalyst

## 和其它工作不同

## Spark SQL vs 关系数据库

## 为什么放弃Shark？和Shark有什么不同
存在三个挑战
1. 仅能查询Hive的catalog数据，不能查询Spark程序内数据，即RDD
2. 必须通过SQL字符串调用Shark，不利于错误发现
3. Hive优化器是针对MapReduce的，很难扩展新的功能，比如用于机器学习的数据类型或添加新的数据源

## 编程接口
### 数据模型
- 支持所有基本SQL数据类型
- 支持复杂数据类型，比如：结构体、数组、maps
- 支持用户自定义数据类型
使用这些数据类型，用户可以从多种数据源和数据格式进行建模，包括Hive、关系数据库、JSON、java/scala/python等中对象

### 操作
- 支持所有的常用关系操作符

## 优化器
### 为什么做优化器？
1. 希望可以容易添加新的优化技术或特性来处理大数据中的各种问题（半结构化数据和高级数据分析）
2. 可以使外部开发者扩展优化器，比如：添加数据源或支持新的数据类型
